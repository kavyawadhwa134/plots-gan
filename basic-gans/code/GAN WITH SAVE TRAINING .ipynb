{"metadata":{"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\nimport os\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the generator network\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Linear(2, 16),  # Input size is 2 (latent space)\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 16),\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 2)  # Output size should be 2 for 2D data\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n# Define the discriminator network\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Linear(2, 16),  # Input size should match the generated data size\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 16),\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 1)  # Output size is 1 for binary classification\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n# Create instances of the generator and discriminator and move to device\ngenerator = Generator().to(device)\ndiscriminator = Discriminator().to(device)\n\n# Loss function\ncriterion = nn.BCEWithLogitsLoss()\n\n# Optimizers\ngen_optimizer = optim.RMSprop(generator.parameters(), lr=0.001)\ndisc_optimizer = optim.RMSprop(discriminator.parameters(), lr=0.001)\n\n# Function to generate noise\ndef sample_Z(m, n):\n    return torch.Tensor(np.random.uniform(-1., 1., size=[m, n])).to(device)\n\n# Load training data from m_hist.txt\ndef load_data(file_path):\n    # Load only the first two columns to avoid dimension mismatch\n    data = np.loadtxt(file_path, usecols=[0, 1])  \n    return torch.Tensor(data).to(device)\n\n# Load your real training data\nreal_data_file = '/home/jovyan/plots-gan/basic-gans/code/m_hist.txt'  # Full path to your data file\nx_plot = load_data(real_data_file)\n\n# Training parameters\nbatch_size = min(256, len(x_plot))  # Adjust batch size to be smaller than or equal to the number of data points\nnd_steps = 10\nng_steps = 10\nnum_iterations = 12001  # Total number of iterations\n\n# Paths to save and load model checkpoints\ncheckpoint_dir = '../plots/checkpoints'\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n# Path to the loss log file\nloss_log_file = '../plots/loss_log.csv'\n\n# Write the header of the CSV file\nwith open(loss_log_file, 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow(['Iteration', 'Discriminator Loss', 'Generator Loss'])\n\n# Load the checkpoint if it exists\nstart_iteration = 0\ncheckpoint_path = os.path.join(checkpoint_dir, 'gan_checkpoint.pth')\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path)\n    generator.load_state_dict(checkpoint['generator_state_dict'])\n    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n    gen_optimizer.load_state_dict(checkpoint['gen_optimizer_state_dict'])\n    disc_optimizer.load_state_dict(checkpoint['disc_optimizer_state_dict'])\n    start_iteration = checkpoint['iteration'] + 1\n    print(f\"Resumed training from iteration {start_iteration}\")\n\n# Training loop\nfor i in range(start_iteration, num_iterations):\n    for _ in range(nd_steps):\n        # Sample a batch of real data\n        indices = np.random.choice(len(x_plot), batch_size, replace=False)\n        X_batch = x_plot[indices]\n        Z_batch = sample_Z(batch_size, 2)\n\n        # Real data labels are 1, generated data labels are 0\n        real_labels = torch.ones(batch_size, 1).to(device)\n        fake_labels = torch.zeros(batch_size, 1).to(device)\n\n        # Compute discriminator loss on real data\n        real_outputs = discriminator(X_batch)\n        real_loss = criterion(real_outputs, real_labels)\n\n        # Compute discriminator loss on fake data\n        fake_data = generator(Z_batch)\n        fake_outputs = discriminator(fake_data.detach())\n        fake_loss = criterion(fake_outputs, fake_labels)\n\n        # Total discriminator loss\n        disc_loss = real_loss + fake_loss\n\n        # Backpropagation and optimization for discriminator\n        disc_optimizer.zero_grad()\n        disc_loss.backward()\n        disc_optimizer.step()\n\n    for _ in range(ng_steps):\n        Z_batch = sample_Z(batch_size, 2)\n        fake_data = generator(Z_batch)\n        fake_outputs = discriminator(fake_data)\n\n        # Generator loss\n        gen_loss = criterion(fake_outputs, real_labels)\n\n        # Backpropagation and optimization for generator\n        gen_optimizer.zero_grad()\n        gen_loss.backward()\n        gen_optimizer.step()\n\n    # Save the losses to the CSV file\n    with open(loss_log_file, 'a', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow([i, disc_loss.item(), gen_loss.item()])\n\n    print(f\"Iterations: {i}\\t Discriminator loss: {disc_loss.item():.4f}\\t Generator loss: {gen_loss.item():.4f}\")\n\n    # Save checkpoint every 1000 iterations\n    if i % 1000 == 0:\n        torch.save({\n            'iteration': i,\n            'generator_state_dict': generator.state_dict(),\n            'discriminator_state_dict': discriminator.state_dict(),\n            'gen_optimizer_state_dict': gen_optimizer.state_dict(),\n            'disc_optimizer_state_dict': disc_optimizer.state_dict()\n        }, checkpoint_path)\n\n        # Plotting\n        plt.figure()\n        with torch.no_grad():\n            g_plot = generator(sample_Z(batch_size, 2)).cpu().numpy()  # Generate new data for plotting\n        plt.scatter(x_plot.cpu()[:, 0], x_plot.cpu()[:, 1], color='blue', label='Real Data', alpha=0.6)\n        plt.scatter(g_plot[:, 0], g_plot[:, 1], color='orange', label='Generated Data', alpha=0.6)\n\n        plt.legend()\n        plt.title(f'Samples at Iteration {i}', fontsize=14)\n        plt.xlabel('X-axis Label', fontsize=12)\n        plt.ylabel('Y-axis Label', fontsize=12)\n        plt.grid(True)\n        plt.tight_layout()\n        plt.savefig(f'../plots/iterations/iteration_{i}.png', dpi=300)\n        plt.close()\n\n# Generate output after training\nwith torch.no_grad():\n    g_plot = generator(sample_Z(batch_size, 2)).cpu().numpy()  # Generate new data for plotting\nplt.figure()\nplt.scatter(x_plot.cpu()[:, 0], x_plot.cpu()[:, 1], color='blue', label='Real Data', alpha=0.6)\nplt.scatter(g_plot[:, 0], g_plot[:, 1], color='orange', label='Generated Data', alpha=0.6)\nplt.legend()\nplt.title('Final Generated Data')\nplt.xlabel('X-axis Label', fontsize=12)\nplt.ylabel('Y-axis Label', fontsize=12)\nplt.grid(True)\nplt.tight_layout()\nplt.savefig('../plots/iterations/final_generated_data.png', dpi=300)\nplt.close()\n","metadata":{"trusted":true,"tags":[]},"execution_count":null,"outputs":[{"name":"stdout","text":"Iterations: 0\t Discriminator loss: 0.5512\t Generator loss: 0.7561\nIterations: 1\t Discriminator loss: 0.5222\t Generator loss: 0.8226\nIterations: 2\t Discriminator loss: 0.4718\t Generator loss: 0.9459\nIterations: 3\t Discriminator loss: 0.4074\t Generator loss: 1.0818\nIterations: 4\t Discriminator loss: 0.3722\t Generator loss: 1.2088\nIterations: 5\t Discriminator loss: 0.3522\t Generator loss: 1.2692\nIterations: 6\t Discriminator loss: 0.3598\t Generator loss: 1.3616\nIterations: 7\t Discriminator loss: 0.3873\t Generator loss: 1.3403\nIterations: 8\t Discriminator loss: 0.4252\t Generator loss: 1.3777\nIterations: 9\t Discriminator loss: 0.5180\t Generator loss: 1.5029\nIterations: 10\t Discriminator loss: 0.5311\t Generator loss: 1.3715\nIterations: 11\t Discriminator loss: 0.6000\t Generator loss: 1.3370\nIterations: 12\t Discriminator loss: 0.6221\t Generator loss: 1.3545\nIterations: 13\t Discriminator loss: 0.6805\t Generator loss: 1.3427\nIterations: 14\t Discriminator loss: 0.7859\t Generator loss: 1.3546\nIterations: 15\t Discriminator loss: 0.8532\t Generator loss: 1.1829\nIterations: 16\t Discriminator loss: 0.8493\t Generator loss: 1.1218\nIterations: 17\t Discriminator loss: 0.8618\t Generator loss: 1.1405\nIterations: 18\t Discriminator loss: 0.8590\t Generator loss: 1.2547\nIterations: 19\t Discriminator loss: 0.8387\t Generator loss: 1.4229\nIterations: 20\t Discriminator loss: 0.7894\t Generator loss: 1.5425\nIterations: 21\t Discriminator loss: 0.7244\t Generator loss: 1.6538\nIterations: 22\t Discriminator loss: 0.6823\t Generator loss: 1.8802\nIterations: 23\t Discriminator loss: 0.6045\t Generator loss: 1.9822\nIterations: 24\t Discriminator loss: 0.5407\t Generator loss: 2.0366\nIterations: 25\t Discriminator loss: 0.4929\t Generator loss: 1.1541\nIterations: 26\t Discriminator loss: 0.6284\t Generator loss: 0.8404\nIterations: 27\t Discriminator loss: 0.8562\t Generator loss: 0.9662\nIterations: 28\t Discriminator loss: 1.0262\t Generator loss: 1.0515\nIterations: 29\t Discriminator loss: 1.1436\t Generator loss: 1.0570\nIterations: 30\t Discriminator loss: 1.2075\t Generator loss: 1.1275\nIterations: 31\t Discriminator loss: 1.2230\t Generator loss: 1.1618\nIterations: 32\t Discriminator loss: 1.2049\t Generator loss: 1.2564\nIterations: 33\t Discriminator loss: 1.1477\t Generator loss: 1.3260\nIterations: 34\t Discriminator loss: 1.0737\t Generator loss: 1.4127\nIterations: 35\t Discriminator loss: 0.9851\t Generator loss: 1.4361\nIterations: 36\t Discriminator loss: 0.9089\t Generator loss: 1.4609\nIterations: 37\t Discriminator loss: 0.8398\t Generator loss: 1.5476\nIterations: 38\t Discriminator loss: 0.7882\t Generator loss: 1.5665\nIterations: 39\t Discriminator loss: 0.7366\t Generator loss: 1.6558\nIterations: 40\t Discriminator loss: 0.7087\t Generator loss: 1.1774\nIterations: 41\t Discriminator loss: 0.7593\t Generator loss: 1.0683\nIterations: 42\t Discriminator loss: 0.8070\t Generator loss: 1.0943\nIterations: 43\t Discriminator loss: 0.8132\t Generator loss: 1.3349\nIterations: 44\t Discriminator loss: 0.7815\t Generator loss: 1.4268\nIterations: 45\t Discriminator loss: 0.7275\t Generator loss: 1.5145\nIterations: 46\t Discriminator loss: 0.6612\t Generator loss: 1.7861\nIterations: 47\t Discriminator loss: 0.6117\t Generator loss: 1.9184\nIterations: 48\t Discriminator loss: 0.5729\t Generator loss: 1.9264\nIterations: 49\t Discriminator loss: 0.5560\t Generator loss: 1.6535\nIterations: 50\t Discriminator loss: 0.5824\t Generator loss: 1.3325\nIterations: 51\t Discriminator loss: 0.6460\t Generator loss: 1.3009\nIterations: 52\t Discriminator loss: 0.6861\t Generator loss: 1.3472\nIterations: 53\t Discriminator loss: 0.7061\t Generator loss: 1.3964\nIterations: 54\t Discriminator loss: 0.7003\t Generator loss: 1.4603\nIterations: 55\t Discriminator loss: 0.6878\t Generator loss: 1.4884\nIterations: 56\t Discriminator loss: 0.6647\t Generator loss: 1.5778\nIterations: 57\t Discriminator loss: 0.6437\t Generator loss: 1.6287\nIterations: 58\t Discriminator loss: 0.6250\t Generator loss: 1.6380\nIterations: 59\t Discriminator loss: 0.6086\t Generator loss: 1.6382\nIterations: 60\t Discriminator loss: 0.6056\t Generator loss: 1.6463\nIterations: 61\t Discriminator loss: 0.5847\t Generator loss: 1.6234\nIterations: 62\t Discriminator loss: 0.5917\t Generator loss: 1.6112\nIterations: 63\t Discriminator loss: 0.5812\t Generator loss: 1.6105\nIterations: 64\t Discriminator loss: 0.5949\t Generator loss: 1.5969\nIterations: 65\t Discriminator loss: 0.6187\t Generator loss: 1.4658\nIterations: 66\t Discriminator loss: 0.6561\t Generator loss: 1.5498\nIterations: 67\t Discriminator loss: 0.6669\t Generator loss: 1.3926\nIterations: 68\t Discriminator loss: 0.6921\t Generator loss: 1.4463\nIterations: 69\t Discriminator loss: 0.7163\t Generator loss: 1.4403\nIterations: 70\t Discriminator loss: 0.7499\t Generator loss: 1.3017\nIterations: 71\t Discriminator loss: 0.7915\t Generator loss: 1.3193\nIterations: 72\t Discriminator loss: 0.8245\t Generator loss: 1.4263\nIterations: 73\t Discriminator loss: 0.8562\t Generator loss: 1.1488\nIterations: 74\t Discriminator loss: 0.9002\t Generator loss: 1.2072\nIterations: 75\t Discriminator loss: 0.9007\t Generator loss: 1.0263\nIterations: 76\t Discriminator loss: 0.9073\t Generator loss: 1.0488\nIterations: 77\t Discriminator loss: 0.9202\t Generator loss: 0.9584\nIterations: 78\t Discriminator loss: 0.9605\t Generator loss: 0.9635\nIterations: 79\t Discriminator loss: 1.0035\t Generator loss: 0.9526\nIterations: 80\t Discriminator loss: 1.0072\t Generator loss: 0.8546\nIterations: 81\t Discriminator loss: 0.9581\t Generator loss: 0.9108\nIterations: 82\t Discriminator loss: 0.9599\t Generator loss: 0.8802\nIterations: 83\t Discriminator loss: 0.9641\t Generator loss: 0.9063\nIterations: 84\t Discriminator loss: 0.9689\t Generator loss: 0.8623\nIterations: 85\t Discriminator loss: 0.9748\t Generator loss: 1.0031\nIterations: 86\t Discriminator loss: 0.9971\t Generator loss: 0.8717\nIterations: 87\t Discriminator loss: 1.0130\t Generator loss: 0.8990\nIterations: 88\t Discriminator loss: 0.9929\t Generator loss: 0.9290\nIterations: 89\t Discriminator loss: 1.0099\t Generator loss: 0.9685\nIterations: 90\t Discriminator loss: 1.0151\t Generator loss: 1.1040\nIterations: 91\t Discriminator loss: 1.0465\t Generator loss: 1.2556\n","output_type":"stream"}],"id":"93aa7e86-8bb4-48cf-99e0-27162534ff27"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"1b905038-4b5d-4dc0-afb4-dd4301905956"}]}