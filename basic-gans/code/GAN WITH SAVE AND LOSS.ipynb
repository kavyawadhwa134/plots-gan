{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the generator network\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Linear(2, 16),  # Input size is 2 (latent space)\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 16),\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 2)  # Output size should be 2 for 2D data\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n# Define the discriminator network\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Linear(2, 16),  # Input size should match the generated data size\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 16),\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 1)  # Output size is 1 for binary classification\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n# Create instances of the generator and discriminator and move to device\ngenerator = Generator().to(device)\ndiscriminator = Discriminator().to(device)\n\n# Loss function\ncriterion = nn.BCEWithLogitsLoss()\n\n# Optimizers\ngen_optimizer = optim.RMSprop(generator.parameters(), lr=0.001)\ndisc_optimizer = optim.RMSprop(discriminator.parameters(), lr=0.001)\n\n# Function to generate noise\ndef sample_Z(m, n):\n    return torch.Tensor(np.random.uniform(-1., 1., size=[m, n])).to(device)\n\n# Load training data from m_hist.txt\ndef load_data(file_path):\n    # Load only the first two columns to avoid dimension mismatch\n    data = np.loadtxt(file_path, usecols=[0, 1])  \n    return torch.Tensor(data).to(device)\n\n# Load your real training data\nreal_data_file = '/home/jovyan/plots-gan/basic-gans/code/m_hist.txt'  # Full path to your data file\nx_plot = load_data(real_data_file)\n\n# Training parameters\nbatch_size = min(256, len(x_plot))  # Adjust batch size to be smaller than or equal to the number of data points\nnd_steps = 10\nng_steps = 10\nnum_iterations = 12001  # Set to 12000 iterations\n\n# Path to the loss log file\nloss_log_file = '../plots/loss_log.csv'\n\n# Write the header of the CSV file\nwith open(loss_log_file, 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow(['Iteration', 'Discriminator Loss', 'Generator Loss'])\n\n# Training loop\nfor i in range(num_iterations):\n    for _ in range(nd_steps):\n        # Sample a batch of real data\n        indices = np.random.choice(len(x_plot), batch_size, replace=False)\n        X_batch = x_plot[indices]\n        Z_batch = sample_Z(batch_size, 2)\n\n        # Real data labels are 1, generated data labels are 0\n        real_labels = torch.ones(batch_size, 1).to(device)\n        fake_labels = torch.zeros(batch_size, 1).to(device)\n\n        # Compute discriminator loss on real data\n        real_outputs = discriminator(X_batch)\n        real_loss = criterion(real_outputs, real_labels)\n\n        # Compute discriminator loss on fake data\n        fake_data = generator(Z_batch)\n        fake_outputs = discriminator(fake_data.detach())\n        fake_loss = criterion(fake_outputs, fake_labels)\n\n        # Total discriminator loss\n        disc_loss = real_loss + fake_loss\n\n        # Backpropagation and optimization for discriminator\n        disc_optimizer.zero_grad()\n        disc_loss.backward()\n        disc_optimizer.step()\n\n    for _ in range(ng_steps):\n        Z_batch = sample_Z(batch_size, 2)\n        fake_data = generator(Z_batch)\n        fake_outputs = discriminator(fake_data)\n\n        # Generator loss\n        gen_loss = criterion(fake_outputs, real_labels)\n\n        # Backpropagation and optimization for generator\n        gen_optimizer.zero_grad()\n        gen_loss.backward()\n        gen_optimizer.step()\n\n    # Save the losses to the CSV file\n    with open(loss_log_file, 'a', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow([i, disc_loss.item(), gen_loss.item()])\n\n    print(f\"Iterations: {i}\\t Discriminator loss: {disc_loss.item():.4f}\\t Generator loss: {gen_loss.item():.4f}\")\n\n    # Plotting every 1000 iterations\n    if i % 1000 == 0:\n        plt.figure()\n        with torch.no_grad():\n            g_plot = generator(sample_Z(batch_size, 2)).cpu().numpy()  # Generate new data for plotting\n        plt.scatter(x_plot.cpu()[:, 0], x_plot.cpu()[:, 1], color='blue', label='Real Data', alpha=0.6)\n        plt.scatter(g_plot[:, 0], g_plot[:, 1], color='orange', label='Generated Data', alpha=0.6)\n\n        plt.legend()\n        plt.title(f'Samples at Iteration {i}', fontsize=14)\n        plt.xlabel('X-axis Label', fontsize=12)\n        plt.ylabel('Y-axis Label', fontsize=12)\n        plt.grid(True)\n        plt.tight_layout()\n        plt.savefig(f'../plots/iterations/iteration_{i}.png', dpi=300)\n        plt.close()\n","metadata":{"tags":[]},"execution_count":null,"outputs":[],"id":"a7d39ded-2836-46a4-850f-72181eae8eb0"},{"cell_type":"code","source":"","metadata":{"trusted":true,"tags":[]},"execution_count":null,"outputs":[],"id":"173f5b7f-ac97-4e3b-a5b1-7b7ed963a301"},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\nimport os\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the generator network\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Linear(2, 16),  # Input size is 2 (latent space)\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 16),\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 2)  # Output size should be 2 for 2D data\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n# Define the discriminator network\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Linear(2, 16),  # Input size should match the generated data size\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 16),\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 1)  # Output size is 1 for binary classification\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n# Create instances of the generator and discriminator and move to device\ngenerator = Generator().to(device)\ndiscriminator = Discriminator().to(device)\n\n# Loss function\ncriterion = nn.BCEWithLogitsLoss()\n\n# Optimizers\ngen_optimizer = optim.RMSprop(generator.parameters(), lr=0.001)\ndisc_optimizer = optim.RMSprop(discriminator.parameters(), lr=0.001)\n\n# Function to generate noise\ndef sample_Z(m, n):\n    return torch.Tensor(np.random.uniform(-1., 1., size=[m, n])).to(device)\n\n# Load training data from m_hist.txt\ndef load_data(file_path):\n    # Load only the first two columns to avoid dimension mismatch\n    data = np.loadtxt(file_path, usecols=[0, 1])  \n    return torch.Tensor(data).to(device)\n\n# Load your real training data\nreal_data_file = '/home/jovyan/plots-gan/basic-gans/code/m_hist.txt'  # Full path to your data file\nx_plot = load_data(real_data_file)\n\n# Training parameters\nbatch_size = min(256, len(x_plot))  # Adjust batch size to be smaller than or equal to the number of data points\nnd_steps = 10\nng_steps = 10\nnum_iterations = 12001  # Set to 12000 iterations\n\n# Path to the loss log file\nloss_log_file = '../plots/loss_log.csv'\n\n# Create a directory for saving checkpoints if it doesn't exist\ncheckpoint_dir = '../plots/checkpoint/'\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Function to save model checkpoint\ndef save_checkpoint(iteration):\n    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_{iteration}.tar\")\n    torch.save({\n        'iteration': iteration,\n        'generator_state_dict': generator.state_dict(),\n        'discriminator_state_dict': discriminator.state_dict(),\n        'gen_optimizer_state_dict': gen_optimizer.state_dict(),\n        'disc_optimizer_state_dict': disc_optimizer.state_dict(),\n    }, checkpoint_path)\n    print(f\"Checkpoint saved at iteration {iteration}\")\n\n# Function to load model checkpoint\ndef load_checkpoint():\n    # Find the latest checkpoint\n    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.tar')]\n    if not checkpoint_files:\n        print(\"No checkpoints found.\")\n        return 0  # No checkpoints found, start from iteration 0\n\n    latest_checkpoint = max(checkpoint_files, key=lambda f: int(f.split('_')[1].split('.')[0]))\n    checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n    \n    checkpoint = torch.load(checkpoint_path)\n    generator.load_state_dict(checkpoint['generator_state_dict'])\n    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n    gen_optimizer.load_state_dict(checkpoint['gen_optimizer_state_dict'])\n    disc_optimizer.load_state_dict(checkpoint['disc_optimizer_state_dict'])\n    print(f\"Checkpoint loaded from iteration {checkpoint['iteration']}\")\n    return checkpoint['iteration']\n\n# Load the last checkpoint if exists\nstart_iteration = load_checkpoint()\n\n# Write the header of the CSV file if starting fresh\nif start_iteration == 0:\n    with open(loss_log_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Iteration', 'Discriminator Loss', 'Generator Loss'])\n\n# Training loop\nfor i in range(start_iteration, num_iterations):\n    for _ in range(nd_steps):\n        # Sample a batch of real data\n        indices = np.random.choice(len(x_plot), batch_size, replace=False)\n        X_batch = x_plot[indices]\n        Z_batch = sample_Z(batch_size, 2)\n\n        # Real data labels are 1, generated data labels are 0\n        real_labels = torch.ones(batch_size, 1).to(device)\n        fake_labels = torch.zeros(batch_size, 1).to(device)\n\n        # Compute discriminator loss on real data\n        real_outputs = discriminator(X_batch)\n        real_loss = criterion(real_outputs, real_labels)\n\n        # Compute discriminator loss on fake data\n        fake_data = generator(Z_batch)\n        fake_outputs = discriminator(fake_data.detach())\n        fake_loss = criterion(fake_outputs, fake_labels)\n\n        # Total discriminator loss\n        disc_loss = real_loss + fake_loss\n\n        # Backpropagation and optimization for discriminator\n        disc_optimizer.zero_grad()\n        disc_loss.backward()\n        disc_optimizer.step()\n\n    for _ in range(ng_steps):\n        Z_batch = sample_Z(batch_size, 2)\n        fake_data = generator(Z_batch)\n        fake_outputs = discriminator(fake_data)\n\n        # Generator loss\n        gen_loss = criterion(fake_outputs, real_labels)\n\n        # Backpropagation and optimization for generator\n        gen_optimizer.zero_grad()\n        gen_loss.backward()\n        gen_optimizer.step()\n\n    # Save the losses to the CSV file\n    with open(loss_log_file, 'a', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow([i, disc_loss.item(), gen_loss.item()])\n\n    print(f\"Iterations: {i}\\t Discriminator loss: {disc_loss.item():.4f}\\t Generator loss: {gen_loss.item():.4f}\")\n\n    # Save models and optimizers as tar files every 1000 iterations\n    if i % 1000 == 0 and i != start_iteration:\n        save_checkpoint(i)\n\n    # Plotting every 1000 iterations\n    if i % 1000 == 0:\n        plt.figure()\n        with torch.no_grad():\n            g_plot = generator(sample_Z(batch_size, 2)).cpu().numpy()  # Generate new data for plotting\n        plt.scatter(x_plot.cpu()[:, 0], x_plot.cpu()[:, 1], color='blue', label='Real Data', alpha=0.6)\n        plt.scatter(g_plot[:, 0], g_plot[:, 1], color='orange', label='Generated Data', alpha=0.6)\n\n        plt.legend()\n        plt.title(f'Samples at Iteration {i}', fontsize=14)\n        plt.xlabel('X-axis Label', fontsize=12)\n        plt.ylabel('Y-axis Label', fontsize=12)\n        plt.grid(True)\n        plt.tight_layout()\n        plt.savefig(f'../plots/iterations/iteration_{i}.png', dpi=300)\n        plt.close()\n","metadata":{"trusted":true,"tags":[]},"execution_count":null,"outputs":[],"id":"cdf279c7-b782-4a39-8b67-3ecd544888f0"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"83408563-604b-41b8-b569-4e98e8f5eb0c"}]}
